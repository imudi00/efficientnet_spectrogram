{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d655d9",
   "metadata": {},
   "source": [
    "## **efficient net + triplet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66fde2b",
   "metadata": {},
   "source": [
    "구조 커스텀에 용이하기 위해 파이토치로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad11a0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071225de",
   "metadata": {},
   "source": [
    "## **모델 (efficient + triplet)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db03651",
   "metadata": {},
   "source": [
    "필요 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a96c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#신경망(Neural Network) 관련 기능을 제공하는 모듈. \n",
    "#레이어(layer), 활성화 함수(activation), 손실 함수(loss), 신경망 구성 요소 포함\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#코사인 유사도 계산\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d70ac7",
   "metadata": {},
   "source": [
    "efficient net -> 임베딩 벡터 변환 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super().__init__()\n",
    "        self.base_model = models.efficientnet_b0(pretrained=True)  # 사전학습 EfficientNet. imagenet 사용(논문과동일)\n",
    "        self.features = self.base_model.features  # 분류기(fc) 제거, feature extractor 부분만 사용\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # 마지막 feature map에 global average pooling\n",
    "                                            # feature map을 한줄 벡터로 압축해야해서...\n",
    "                                            \n",
    "        self.embedding = nn.Linear(1280, embedding_size)  # 1280채널 → 임베딩 크기(128)로 축소\n",
    "        self.l2_norm = nn.functional.normalize  # 임베딩 벡터 정규화 함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # 이미지 특징 추출\n",
    "        x = self.pool(x)  # 채널별 평균값으로 차원 축소\n",
    "        x = torch.flatten(x, 1)  # 2D → 1D 벡터로 변환\n",
    "        x = self.embedding(x)  # 임베딩 벡터 생성\n",
    "        x = self.l2_norm(x, dim=1)  # 임베딩 벡터 정규화 (길이 1로)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf9bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\.conda\\envs\\tf-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\.conda\\envs\\tf-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNetEmbedding(embedding_size=128) # ✅ 임베딩 바꾸기 가능. 현재 128\n",
    "\n",
    "loss_fn = nn.TripletMarginLoss(margin=1.0)  # Triplet Loss 함수 (margin은 거리 차이 최소 기준)\n",
    "#앵커와 음성 간 거리가 앵커와 양성 간 거리보다 최소 1.0 이상 더 커야 손실이 0이 되고 학습이 멈춤 (조건 만족)\n",
    "#만약 두 거리 차이가 margin보다 작으면 손실이 양수이고, 모델은 차이를 늘리려고 학습함\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Triplet Dataset 클래스 정의. 앵커, 양수, 음수 이미지 선택하는 함수.\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform \n",
    "        # 곡 ID 폴더 리스트\n",
    "        self.song_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        \n",
    "        # 곡별 이미지 리스트 준비\n",
    "        self.data = []\n",
    "        for song_dir in self.song_dirs:\n",
    "            images = [f for f in os.listdir(song_dir) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "            if len(images) >= 2:  # 양성 쌍 생성 가능하려면 2개 이상 필요\n",
    "                self.data.append((song_dir, images))\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 전체 이미지 수 기준 (대략)\n",
    "        return sum(len(images) for _, images in self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 임의로 앵커 곡과 이미지 선택\n",
    "        anchor_song_idx = random.randint(0, len(self.data) - 1)\n",
    "        anchor_song_dir, anchor_images = self.data[anchor_song_idx]\n",
    "\n",
    "        # 앵커 이미지 선택\n",
    "        anchor_img_name = random.choice(anchor_images)\n",
    "        # 양성 이미지(같은 곡 내 다른 이미지) 선택 (본인 제외)\n",
    "        positive_img_name = anchor_img_name\n",
    "        while positive_img_name == anchor_img_name:\n",
    "            positive_img_name = random.choice(anchor_images)\n",
    "\n",
    "        # 음성 곡과 이미지 선택 (다른 곡)\n",
    "        negative_song_idx = anchor_song_idx\n",
    "        while negative_song_idx == anchor_song_idx:\n",
    "            negative_song_idx = random.randint(0, len(self.data) -1)\n",
    "        negative_song_dir, negative_images = self.data[negative_song_idx]\n",
    "        negative_img_name = random.choice(negative_images)\n",
    "\n",
    "        # 이미지 로드 및 transform 적용\n",
    "        anchor_img = Image.open(os.path.join(anchor_song_dir, anchor_img_name)).convert('RGB')\n",
    "        positive_img = Image.open(os.path.join(anchor_song_dir, positive_img_name)).convert('RGB')\n",
    "        negative_img = Image.open(os.path.join(negative_song_dir, negative_img_name)).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            positive_img = self.transform(positive_img)\n",
    "            negative_img = self.transform(negative_img)\n",
    "\n",
    "        return anchor_img, positive_img, negative_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbbb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) 이미지 전처리 설정 (EfficientNet 사전학습 기준) ✅✅✅✅증강 처리 필요!!\n",
    "transform = transforms.Compose([\n",
    "    #전에 하던 증강이랑 똑같이 처리.\n",
    "        transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1)\n",
    "    ),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) 데이터셋과 데이터로더 생성\n",
    "root_data_dir = \"./data\"  # ✅ 수정\n",
    "triplet_dataset = TripletDataset(root_dir=root_data_dir, transform=transform)\n",
    "data_loader = DataLoader(triplet_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8dd251",
   "metadata": {},
   "source": [
    "triplet loss 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b719e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triplet(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    total_loss = 0  # 손실 합산용 변수 초기화\n",
    "    \n",
    "    for anchor, positive, negative in data_loader:  # 데이터로더에서 triplet 배치 단위로 불러오기\n",
    "        anchor = anchor.to(device)      # 앵커 이미지 배치를 GPU/CPU에 올림\n",
    "        positive = positive.to(device)  # 양성 이미지 배치를 GPU/CPU에 올림\n",
    "        negative = negative.to(device)  # 음성 이미지 배치를 GPU/CPU에 올림\n",
    "\n",
    "        optimizer.zero_grad()  # 이전 배치의 기울기 초기화\n",
    "        \n",
    "        # 각 배치에 대해 임베딩 벡터 생성\n",
    "        anchor_embed = model(anchor)      \n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        \n",
    "        # Triplet Loss 계산 (앵커-양성은 가깝게, 앵커-음성은 멀게)\n",
    "        loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        \n",
    "        # 손실값을 기준으로 역전파 (모델 가중치 업데이트 방향 계산)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 옵티마이저로 가중치 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 배치 손실값을 누적\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 전체 데이터셋 평균 손실값 반환\n",
    "    return total_loss / len(data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a5f66",
   "metadata": {},
   "source": [
    "데이터를 batch 단위로 받아서 EfficientNet 모델로 임베딩 벡터를 뽑고\n",
    "\n",
    "Triplet Loss 함수로 거리 차이를 계산해서 손실값을 구함\n",
    "\n",
    "역전파로 모델 파라미터를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc93f5",
   "metadata": {},
   "source": [
    "## **갤러리, 테스트 임베딩 추출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded0be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, inputs, device, batch_size=64):\n",
    "    \"\"\"\n",
    "    inputs: DataLoader 또는 Tensor(single or batch)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if isinstance(inputs, DataLoader): #갤러리 일 경우.\n",
    "            for batch in inputs:\n",
    "                batch = batch.to(device)\n",
    "                emb = model(batch)\n",
    "                embeddings.append(emb.cpu().numpy())\n",
    "            embeddings = np.vstack(embeddings)\n",
    "        else: #테스트일 경우.\n",
    "            # 단일 또는 소량 이미지 텐서 처리\n",
    "            inputs = inputs.to(device)\n",
    "            emb = model(inputs)\n",
    "            embeddings = emb.cpu().numpy()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0da726",
   "metadata": {},
   "source": [
    "## **top-k 추천**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e635f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_topk(query_embedding, gallery_embeddings, gallery_ids, topk=5):\n",
    "    sims = cosine_similarity(query_embedding.reshape(1, -1), gallery_embeddings).flatten()\n",
    "    topk_idx = sims.argsort()[::-1][:topk]\n",
    "    return [(gallery_ids[i], sims[i]) for i in topk_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) 학습 실행 예시\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_loss = train_triplet(model, data_loader, optimizer, loss_fn, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # 필요하면 체크포인트 저장\n",
    "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    # 10) 갤러리 임베딩 미리 추출 및 저장 예시\n",
    "    gallery_loader = DataLoader(triplet_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    gallery_embeddings = extract_embedding(model, gallery_loader, device)\n",
    "    np.save(\"gallery_embeddings.npy\", gallery_embeddings)\n",
    "    \n",
    "    # 11) 테스트 쿼리 임베딩과 추천 예시\n",
    "    # (테스트용 단일 이미지 예시)\n",
    "    test_img_path = \"./test_query.png\"  # 사용자 쿼리 이미지 경로로 변경하세요\n",
    "    test_img = Image.open(test_img_path).convert('RGB')\n",
    "    test_img_tensor = transform(test_img).unsqueeze(0).to(device)  # 배치 차원 추가\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(test_img_tensor).cpu().numpy()\n",
    "    \n",
    "    gallery_ids = [d for d, _ in triplet_dataset.data]  # 곡 ID 리스트 (폴더명)\n",
    "    recommendations = recommend_topk(query_embedding, gallery_embeddings, gallery_ids, topk=5)\n",
    "    print(\"추천 결과:\", recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
