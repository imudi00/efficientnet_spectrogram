{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d655d9",
   "metadata": {},
   "source": [
    "## **efficient net + triplet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66fde2b",
   "metadata": {},
   "source": [
    "구조 커스텀에 용이하기 위해 파이토치로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad11a0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\.conda\\envs\\tf-env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071225de",
   "metadata": {},
   "source": [
    "## **1.모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db03651",
   "metadata": {},
   "source": [
    "필요 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a96c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#신경망(Neural Network) 관련 기능을 제공하는 모듈. \n",
    "#레이어(layer), 활성화 함수(activation), 손실 함수(loss), 신경망 구성 요소 포함\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "#코사인 유사도 계산\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d70ac7",
   "metadata": {},
   "source": [
    "efficient net -> 임베딩 벡터 변환 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super().__init__()\n",
    "        self.base_model = models.efficientnet_b0(pretrained=True)  # 사전학습 EfficientNet. imagenet 사용(논문과동일)\n",
    "        self.features = self.base_model.features  # 분류기(fc) 제거, feature extractor 부분만 사용\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # 마지막 feature map에 global average pooling\n",
    "                                            # feature map을 한줄 벡터로 압축해야해서...\n",
    "                                            \n",
    "        self.embedding = nn.Linear(1280, embedding_size)  # 1280채널 → 임베딩 크기(128)로 축소\n",
    "        self.l2_norm = nn.functional.normalize  # 임베딩 벡터 정규화 함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # 이미지 특징 추출\n",
    "        x = self.pool(x)  # 채널별 평균값으로 차원 축소\n",
    "        x = torch.flatten(x, 1)  # 2D → 1D 벡터로 변환\n",
    "        x = self.embedding(x)  # 임베딩 벡터 생성\n",
    "        x = self.l2_norm(x, dim=1)  # 임베딩 벡터 정규화 (길이 1로)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf9bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\.conda\\envs\\tf-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\.conda\\envs\\tf-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to C:\\Users\\Administrator/.cache\\torch\\hub\\checkpoints\\efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20.5M/20.5M [01:34<00:00, 228kB/s] \n"
     ]
    }
   ],
   "source": [
    "model = EfficientNetEmbedding(embedding_size=128)\n",
    "\n",
    "loss_fn = nn.TripletMarginLoss(margin=1.0)  # Triplet Loss 함수 (margin은 거리 차이 최소 기준)\n",
    "#앵커와 음성 간 거리가 앵커와 양성 간 거리보다 최소 1.0 이상 더 커야\n",
    "#손실이 0이 되고 학습이 멈춤 (조건 만족)\n",
    "#만약 두 거리 차이가 margin보다 작으면 손실이 양수이고, 모델은 차이를 늘리려고 학습함\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8dd251",
   "metadata": {},
   "source": [
    "triplet loss 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triplet(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    total_loss = 0  # 손실 합산용 변수 초기화\n",
    "    \n",
    "    for anchor, positive, negative in data_loader:  # 데이터로더에서 triplet 배치 단위로 불러오기\n",
    "        anchor = anchor.to(device)      # 앵커 이미지 배치를 GPU/CPU에 올림\n",
    "        positive = positive.to(device)  # 양성 이미지 배치를 GPU/CPU에 올림\n",
    "        negative = negative.to(device)  # 음성 이미지 배치를 GPU/CPU에 올림\n",
    "\n",
    "        optimizer.zero_grad()  # 이전 배치의 기울기 초기화\n",
    "        \n",
    "        # 각 배치에 대해 임베딩 벡터 생성\n",
    "        anchor_embed = model(anchor)      \n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        \n",
    "        # Triplet Loss 계산 (앵커-양성은 가깝게, 앵커-음성은 멀게)\n",
    "        loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        \n",
    "        # 손실값을 기준으로 역전파 (모델 가중치 업데이트 방향 계산)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 옵티마이저로 가중치 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 배치 손실값을 누적\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 전체 데이터셋 평균 손실값 반환\n",
    "    return total_loss / len(data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a5f66",
   "metadata": {},
   "source": [
    "데이터를 batch 단위로 받아서 EfficientNet 모델로 임베딩 벡터를 뽑고\n",
    "\n",
    "Triplet Loss 함수로 거리 차이를 계산해서 손실값을 구함\n",
    "\n",
    "역전파로 모델 파라미터를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc93f5",
   "metadata": {},
   "source": [
    "## **2.갤러리 임베딩 추출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cede16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(model, data_loader, device):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            x = x.to(device)\n",
    "            emb = model(x)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8dc1d",
   "metadata": {},
   "source": [
    "## **3.테스트 임베딩 추출**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0da726",
   "metadata": {},
   "source": [
    "## **4.top-k 추천**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
