{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "457f675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: 12.1\n",
      "is_available: True\n",
      "Device: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA:\", torch.version.cuda)\n",
    "print(\"is_available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42eda66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ÏÇ¨Ïö© Í∞ÄÎä•: True\n",
      "GPU Ïù¥Î¶Ñ: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA ÏÇ¨Ïö© Í∞ÄÎä•:\", torch.cuda.is_available())\n",
    "print(\"GPU Ïù¥Î¶Ñ:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a53eab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ ÏÖÄ 1: Î™®Îìà import Î∞è Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "data_root = r\"C:\\ts_data_triplet_split\\train\"\n",
    "song_ids = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])\n",
    "random.seed(42)\n",
    "random.shuffle(song_ids)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(song_ids) * split_ratio)\n",
    "train_ids = song_ids[:split_index]\n",
    "val_ids = song_ids[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a98a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ ÏÖÄ 2: EfficientNet ÏûÑÎ≤†Îî© Î™®Îç∏ Ï†ïÏùò\n",
    "class EfficientNetEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super().__init__()\n",
    "        self.base_model = models.efficientnet_b0(pretrained=True)\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.features = self.base_model.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.embedding = nn.Linear(1280, embedding_size)\n",
    "        self.l2_norm = nn.functional.normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x = self.l2_norm(x, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99d4d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ ÏÖÄ 3: Triplet Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, song_ids=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if song_ids is None:\n",
    "            self.song_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        else:\n",
    "            self.song_dirs = [os.path.join(root_dir, d) for d in song_ids]\n",
    "        self.data = []\n",
    "        for song_dir in self.song_dirs:\n",
    "            images = [f for f in os.listdir(song_dir) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "            if len(images) >= 2:\n",
    "                self.data.append((song_dir, images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(images) for _, images in self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = time.time()\n",
    "\n",
    "        anchor_song_idx = random.randint(0, len(self.data) - 1)\n",
    "        anchor_song_dir, anchor_images = self.data[anchor_song_idx]\n",
    "        anchor_img_name = random.choice(anchor_images)\n",
    "        positive_img_name = anchor_img_name\n",
    "        while positive_img_name == anchor_img_name:\n",
    "            positive_img_name = random.choice(anchor_images)\n",
    "        negative_song_idx = anchor_song_idx\n",
    "        while negative_song_idx == anchor_song_idx:\n",
    "            negative_song_idx = random.randint(0, len(self.data) - 1)\n",
    "        negative_song_dir, negative_images = self.data[negative_song_idx]\n",
    "        negative_img_name = random.choice(negative_images)\n",
    "\n",
    "        anchor_img = Image.open(os.path.join(anchor_song_dir, anchor_img_name)).convert('RGB')\n",
    "        positive_img = Image.open(os.path.join(anchor_song_dir, positive_img_name)).convert('RGB')\n",
    "        negative_img = Image.open(os.path.join(negative_song_dir, negative_img_name)).convert('RGB')\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            positive_img = self.transform(positive_img)\n",
    "            negative_img = self.transform(negative_img)\n",
    "        print(f\"ÏÉòÌîå Î°úÎî© ÏãúÍ∞Ñ: {time.time() - start:.4f}Ï¥à\")\n",
    "        return anchor_img, positive_img, negative_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "957b551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ ÏÖÄ 4: Ï†ÑÏ≤òÎ¶¨, Îç∞Ïù¥ÌÑ∞ÏÖã Î∞è Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÑ§Ï†ï\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = TripletDataset(root_dir=data_root, song_ids=train_ids, transform=transform)\n",
    "val_dataset = TripletDataset(root_dir=data_root, song_ids=val_ids, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,      # üöÄ Î≥ëÎ†¨ Ïù¥ÎØ∏ÏßÄ Î°úÎî©\n",
    "    pin_memory=True     # üöÄ GPU Ï†ÑÏÜ° ÏÜçÎèÑ ÏµúÏ†ÅÌôî\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2,      # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Îäî Ï°∞Í∏à Ï†ÅÏñ¥ÎèÑ OK\n",
    "    pin_memory=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110c5b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triplet(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (anchor, positive, negative) in enumerate(data_loader):\n",
    "        print(f\"Batch {i+1} ÏãúÏûë\")  # Î∞∞Ïπò Î≤àÌò∏ Ï∂úÎ†•\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        anchor_embed = model(anchor)\n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Batch {i+1} ÏôÑÎ£å, Loss: {loss.item():.4f}\")  # Î∞∞Ïπò ÎÅùÎÇ† Îïå Ï∂úÎ†•\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_triplet(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for anchor, positive, negative in data_loader:\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        anchor_embed = model(anchor)\n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ee36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed2e344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: 12.1\n",
      "is_available: True\n",
      "Device: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA:\", torch.version.cuda)  # ‚úÖ\n",
    "print(\"is_available:\", torch.cuda.is_available())  # ‚úÖ\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))  # ‚úÖ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586f1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\anaconda3\\envs\\mynev\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\anaconda3\\envs\\mynev\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ ÏÖÄ 6: ÌïôÏäµ Ïã§Ìñâ\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientNetEmbedding(embedding_size=128).to(device)\n",
    "loss_fn = nn.TripletMarginLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_triplet(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss = validate_triplet(model, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e69ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ ÏÖÄ 7: ÏûÑÎ≤†Îî© Ï∂îÏ∂ú Î∞è Ï∂îÏ≤ú Ìï®Ïàò\n",
    "def extract_embeddings(model, inputs, device, batch_size=64):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        if isinstance(inputs, DataLoader):\n",
    "            for batch in inputs:\n",
    "                batch = batch.to(device)\n",
    "                emb = model(batch)\n",
    "                embeddings.append(emb.cpu().numpy())\n",
    "            embeddings = np.vstack(embeddings)\n",
    "        else:\n",
    "            inputs = inputs.to(device)\n",
    "            emb = model(inputs)\n",
    "            embeddings = emb.cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "def recommend_topk(query_embedding, gallery_embeddings, gallery_ids, topk=5):\n",
    "    sims = cosine_similarity(query_embedding.reshape(1, -1), gallery_embeddings).flatten()\n",
    "    topk_idx = sims.argsort()[::-1][:topk]\n",
    "    return [(gallery_ids[i], sims[i]) for i in topk_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0fd86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ ÏÖÄ 8: Ï∂îÎ°† Î∞è Ï∂îÏ≤ú Ïã§Ìñâ\n",
    "gallery_ids = train_ids + val_ids\n",
    "gallery_dataset = TripletDataset(root_dir=data_root, song_ids=gallery_ids, transform=transform)\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "gallery_embeddings = extract_embeddings(model, gallery_loader, device)\n",
    "np.save(\"gallery_embeddings.npy\", gallery_embeddings)\n",
    "\n",
    "test_img_path = r\"C:\\ts_data_triplet_split\\test\"\n",
    "test_img = Image.open(test_img_path).convert('RGB')\n",
    "test_img_tensor = transform(test_img).unsqueeze(0)\n",
    "query_embedding = extract_embeddings(model, test_img_tensor, device)\n",
    "\n",
    "gallery_id_names = [os.path.basename(d) for d, _ in gallery_dataset.data]\n",
    "recommendations = recommend_topk(query_embedding, gallery_embeddings, gallery_id_names, topk=5)\n",
    "\n",
    "print(\"üéß Ï∂îÏ≤ú Í≤∞Í≥º:\")\n",
    "for i, (song_id, score) in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. Í≥° ID: {song_id} (Ïú†ÏÇ¨ÎèÑ: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3c392",
   "metadata": {},
   "source": [
    "## **Ï†ÑÏ≤¥ ÏΩîÎìú... ÌååÏùº Íµ¨Ï°∞ ÎÇòÎà†ÏïºÌï®..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\anaconda3\\envs\\mynev\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\anaconda3\\envs\\mynev\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.8948, Val Loss: 0.8582\n",
      "Epoch 2/10, Train Loss: 0.8108, Val Loss: 0.7988\n",
      "Epoch 3/10, Train Loss: 0.7629, Val Loss: 0.7507\n",
      "Epoch 4/10, Train Loss: 0.7410, Val Loss: 0.7102\n",
      "Epoch 5/10, Train Loss: 0.7000, Val Loss: 0.6907\n",
      "Epoch 6/10, Train Loss: 0.6994, Val Loss: 0.7181\n",
      "Epoch 7/10, Train Loss: 0.6782, Val Loss: 0.6610\n",
      "Epoch 8/10, Train Loss: 0.6557, Val Loss: 0.6797\n",
      "Epoch 9/10, Train Loss: 0.6722, Val Loss: 0.6530\n",
      "Epoch 10/10, Train Loss: 0.6613, Val Loss: 0.6443\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ ÏÖÄ 1: Î™®Îìà import Î∞è Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "data_root = r\"C:\\ts_data_triplet_split\\train\"\n",
    "song_ids = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])\n",
    "random.seed(42)\n",
    "random.shuffle(song_ids)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(song_ids) * split_ratio)\n",
    "train_ids = song_ids[:split_index]\n",
    "val_ids = song_ids[split_index:]\n",
    "\n",
    "# ‚úÖ ÏÖÄ 2: EfficientNet ÏûÑÎ≤†Îî© Î™®Îç∏ Ï†ïÏùò\n",
    "class EfficientNetEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super().__init__()\n",
    "        self.base_model = models.efficientnet_b0(pretrained=True)\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.features = self.base_model.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.embedding = nn.Linear(1280, embedding_size)\n",
    "        self.l2_norm = nn.functional.normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x = self.l2_norm(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ‚úÖ ÏÖÄ 3: Triplet Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, song_ids=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if song_ids is None:\n",
    "            self.song_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        else:\n",
    "            self.song_dirs = [os.path.join(root_dir, d) for d in song_ids]\n",
    "        self.data = []\n",
    "        for song_dir in self.song_dirs:\n",
    "            images = [f for f in os.listdir(song_dir) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "            if len(images) >= 2:\n",
    "                self.data.append((song_dir, images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(images) for _, images in self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = time.time()\n",
    "\n",
    "        anchor_song_idx = random.randint(0, len(self.data) - 1)\n",
    "        anchor_song_dir, anchor_images = self.data[anchor_song_idx]\n",
    "        anchor_img_name = random.choice(anchor_images)\n",
    "        positive_img_name = anchor_img_name\n",
    "        while positive_img_name == anchor_img_name:\n",
    "            positive_img_name = random.choice(anchor_images)\n",
    "        negative_song_idx = anchor_song_idx\n",
    "        while negative_song_idx == anchor_song_idx:\n",
    "            negative_song_idx = random.randint(0, len(self.data) - 1)\n",
    "        negative_song_dir, negative_images = self.data[negative_song_idx]\n",
    "        negative_img_name = random.choice(negative_images)\n",
    "\n",
    "        anchor_img = Image.open(os.path.join(anchor_song_dir, anchor_img_name)).convert('RGB')\n",
    "        positive_img = Image.open(os.path.join(anchor_song_dir, positive_img_name)).convert('RGB')\n",
    "        negative_img = Image.open(os.path.join(negative_song_dir, negative_img_name)).convert('RGB')\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            positive_img = self.transform(positive_img)\n",
    "            negative_img = self.transform(negative_img)\n",
    "        return anchor_img, positive_img, negative_img\n",
    "\n",
    "# ‚úÖ ÏÖÄ 4: Ï†ÑÏ≤òÎ¶¨, Îç∞Ïù¥ÌÑ∞ÏÖã Î∞è Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÑ§Ï†ï\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = TripletDataset(root_dir=data_root, song_ids=train_ids, transform=transform)\n",
    "val_dataset = TripletDataset(root_dir=data_root, song_ids=val_ids, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,      # üöÄ Î≥ëÎ†¨ Ïù¥ÎØ∏ÏßÄ Î°úÎî©\n",
    "    pin_memory=True     # üöÄ GPU Ï†ÑÏÜ° ÏÜçÎèÑ ÏµúÏ†ÅÌôî\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,      # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Îäî Ï°∞Í∏à Ï†ÅÏñ¥ÎèÑ OK\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "def train_triplet(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (anchor, positive, negative) in enumerate(data_loader):\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        anchor_embed = model(anchor)\n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_triplet(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for anchor, positive, negative in data_loader:\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        anchor_embed = model(anchor)\n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# ‚úÖ ÏÖÄ 6: ÌïôÏäµ Ïã§Ìñâ -- 6Î≤àÍπåÏßÑ Ïûò ÎèåÏïÑÍ∞ê. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientNetEmbedding(embedding_size=128).to(device)\n",
    "loss_fn = nn.TripletMarginLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "''' ÌïôÏäµ ÏôÑÎ£åÏãú ÌïÑÏöîÏóÜÏùå.\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_triplet(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss = validate_triplet(model, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "'''\n",
    "\n",
    "#ÌïôÏäµ ÏôÑÎ£åÏãú Ï†ÄÏû•Îêú Í∞ÄÏ§ëÏπò Î∂àÎü¨Ïò§Í∏∞.\n",
    "model.load_state_dict(torch.load(\"model_epoch_10.pth\", map_location=device))  # ÌååÏùºÎ™Ö ÌôïÏù∏\n",
    "model.eval()\n",
    "\n",
    "# ‚úÖ ÏÖÄ 7: ÏûÑÎ≤†Îî© Ï∂îÏ∂ú Î∞è Ï∂îÏ≤ú Ìï®Ïàò\n",
    "def extract_embeddings(model, inputs, device, batch_size=64):\n",
    "    print(\"[DEBUG] ‚ñ∂ ÏûÑÎ≤†Îî© Ï∂îÏ∂ú ÏãúÏûë\")\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        if isinstance(inputs, DataLoader):\n",
    "            if len(inputs.dataset) == 0:\n",
    "                print(\"[ERROR] ‚ö†Ô∏è DataLoaderÏùò DatasetÏù¥ ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§.\")\n",
    "                return np.array([])  # üëà Îπà Î∞∞Ïó¥ Î¶¨ÌÑ¥ÌïòÍ±∞ÎÇò Ï¢ÖÎ£å Ï≤òÎ¶¨\n",
    "            \n",
    "            print(f\"[DEBUG] ‚ñ∂ inputs Í∏∏Ïù¥: {len(inputs.dataset)}\")\n",
    "            for batch in inputs:\n",
    "                print(f\"[DEBUG] ‚ñ∂ Î∞∞Ïπò {i+1} Ï∂îÏ∂ú Ï§ë...\")\n",
    "                batch = batch.to(device)\n",
    "                emb = model(batch)\n",
    "                print(f\"[DEBUG] ‚ñ∂ Î∞∞Ïπò {i+1} ÏûÑÎ≤†Îî© shape: {emb.shape}\")\n",
    "                embeddings.append(emb.cpu().numpy())\n",
    "            embeddings = np.vstack(embeddings)\n",
    "            print(f\"[DEBUG] ‚ñ∂ Ï†ÑÏ≤¥ ÏûÑÎ≤†Îî© shape: {embeddings.shape}\")\n",
    "        else:\n",
    "            print(\"[DEBUG] ‚ñ∂ Îã®Ïùº Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî© Ï∂îÏ∂ú\")\n",
    "            inputs = inputs.to(device)\n",
    "            emb = model(inputs)\n",
    "            print(f\"[DEBUG] ‚ñ∂ Îã®Ïùº ÏûÑÎ≤†Îî© shape: {emb.shape}\")\n",
    "            embeddings = emb.cpu().numpy()\n",
    "    print(\"[DEBUG] ‚ñ∂ ÏûÑÎ≤†Îî© Ï∂îÏ∂ú ÏôÑÎ£å\")\n",
    "    return embeddings\n",
    "\n",
    "def recommend_topk(query_embedding, gallery_embeddings, gallery_ids, topk=5):\n",
    "    sims = cosine_similarity(query_embedding.reshape(1, -1), gallery_embeddings).flatten()\n",
    "    topk_idx = sims.argsort()[::-1][:topk]\n",
    "    return [(gallery_ids[i], sims[i]) for i in topk_idx]\n",
    "\n",
    "# ‚úÖ ÏÖÄ 7,8ÏÇ¨Ïù¥ : Ïã±Í∏ÄÎç∞Ïù¥ÌÑ∞ÏÖã ÎßåÎìúÎäî ÌÅ¥ÎûòÏä§ ÌïÑÏöî.\n",
    "class SingleImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, song_ids=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.image_ids = []\n",
    "        song_dirs = [os.path.join(root_dir, d) for d in song_ids]\n",
    "        for song_dir in song_dirs:\n",
    "            images = [f for f in os.listdir(song_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            for img in images:\n",
    "                self.image_paths.append(os.path.join(song_dir, img))\n",
    "                self.image_ids.append(os.path.basename(song_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(f\"[DEBUG] ‚ñ∂ Ïù¥ÎØ∏ÏßÄ Î°úÎìú ÏãúÏûë: {self.image_paths[idx]}\")\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "# ‚úÖ ÏÖÄ 8: Ï∂îÎ°† Î∞è Ï∂îÏ≤ú Ïã§Ìñâ\n",
    "# ‚úÖ Í∞§Îü¨Î¶¨ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ Î∞è ÏûÑÎ≤†Îî© Ï∂îÏ∂ú. ÌïúÎ≤àÎßå Ìï¥ÏÑú Ï†ÄÏû•Ìï¥ÎÜ®Îã§Í∞Ä Ï∂îÏ≤úÌï† Îïå ÏÇ¨Ïö©.\n",
    "gallery_ids = train_ids + val_ids\n",
    "gallery_dataset = SingleImageDataset(root_dir=data_root, song_ids=gallery_ids, transform=transform)\n",
    "\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "gallery_embeddings = extract_embeddings(model, gallery_loader, device)\n",
    "np.save(\"gallery_embeddings.npy\", gallery_embeddings)\n",
    "\n",
    "# ‚úÖ Í∞§Îü¨Î¶¨ ID Ïù¥Î¶Ñ Ï∂îÏ∂ú\n",
    "gallery_id_names = [os.path.basename(d) for d, _ in gallery_dataset.data]\n",
    "\n",
    "# ‚úÖ ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨ ÏßÄÏ†ï\n",
    "test_img_dir = r\"C:\\ts_data_triplet_split\\test\"\n",
    "\n",
    "# ‚úÖ ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄÎì§ Ï§ë Î¨¥ÏûëÏúÑÎ°ú 10Í∞ú ÏÑ†ÌÉù\n",
    "all_test_paths = [\n",
    "    os.path.join(test_img_dir, f)\n",
    "    for f in os.listdir(test_img_dir)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]\n",
    "random.seed(42)\n",
    "test_img_paths = random.sample(all_test_paths, min(10, len(all_test_paths)))\n",
    "\n",
    "# ‚úÖ ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ ÌïòÎÇòÏî© Ï∂îÏ≤ú ÏàòÌñâ\n",
    "for test_img_path in test_img_paths:\n",
    "    print(f\"\\n[DEBUG] ‚ñ∂ ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ: {test_img_path}\")\n",
    "    test_img = Image.open(test_img_path).convert('RGB')\n",
    "    test_img_tensor = transform(test_img).unsqueeze(0)  # (1, C, H, W)\n",
    "    query_embedding = extract_embeddings(model, test_img_tensor, device)\n",
    "\n",
    "    print(\"[DEBUG] ‚ñ∂ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ Ï§ë...\")\n",
    "    recommendations = recommend_topk(query_embedding, gallery_embeddings, gallery_id_names, topk=5)\n",
    "\n",
    "    print(f\"\\nüéß [{os.path.basename(test_img_path)}]Ïóê ÎåÄÌïú Ï∂îÏ≤ú Í≤∞Í≥º:\")\n",
    "    for i, (song_id, score) in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. Í≥° ID: {song_id} (Ïú†ÏÇ¨ÎèÑ: {score:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
