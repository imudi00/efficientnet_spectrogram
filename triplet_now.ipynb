{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "457f675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: 12.1\n",
      "is_available: True\n",
      "Device: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA:\", torch.version.cuda)\n",
    "print(\"is_available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42eda66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "GPU ì´ë¦„: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA ì‚¬ìš© ê°€ëŠ¥:\", torch.cuda.is_available())\n",
    "print(\"GPU ì´ë¦„:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a53eab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì…€ 1: ëª¨ë“ˆ import ë° ê²½ë¡œ ì„¤ì •\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "data_root = r\"C:\\ts_data_triplet_split\\train\"\n",
    "song_ids = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])\n",
    "random.seed(42)\n",
    "random.shuffle(song_ids)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(song_ids) * split_ratio)\n",
    "train_ids = song_ids[:split_index]\n",
    "val_ids = song_ids[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a98a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì…€ 2: EfficientNet ì„ë² ë”© ëª¨ë¸ ì •ì˜\n",
    "class EfficientNetEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super().__init__()\n",
    "        self.base_model = models.efficientnet_b0(pretrained=True)\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.features = self.base_model.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.embedding = nn.Linear(1280, embedding_size)\n",
    "        self.l2_norm = nn.functional.normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x = self.l2_norm(x, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99d4d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì…€ 3: Triplet Dataset í´ë˜ìŠ¤ ì •ì˜\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, song_ids=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if song_ids is None:\n",
    "            self.song_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        else:\n",
    "            self.song_dirs = [os.path.join(root_dir, d) for d in song_ids]\n",
    "        self.data = []\n",
    "        for song_dir in self.song_dirs:\n",
    "            images = [f for f in os.listdir(song_dir) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "            if len(images) >= 2:\n",
    "                self.data.append((song_dir, images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(images) for _, images in self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = time.time()\n",
    "\n",
    "        anchor_song_idx = random.randint(0, len(self.data) - 1)\n",
    "        anchor_song_dir, anchor_images = self.data[anchor_song_idx]\n",
    "        anchor_img_name = random.choice(anchor_images)\n",
    "        positive_img_name = anchor_img_name\n",
    "        while positive_img_name == anchor_img_name:\n",
    "            positive_img_name = random.choice(anchor_images)\n",
    "        negative_song_idx = anchor_song_idx\n",
    "        while negative_song_idx == anchor_song_idx:\n",
    "            negative_song_idx = random.randint(0, len(self.data) - 1)\n",
    "        negative_song_dir, negative_images = self.data[negative_song_idx]\n",
    "        negative_img_name = random.choice(negative_images)\n",
    "\n",
    "        anchor_img = Image.open(os.path.join(anchor_song_dir, anchor_img_name)).convert('RGB')\n",
    "        positive_img = Image.open(os.path.join(anchor_song_dir, positive_img_name)).convert('RGB')\n",
    "        negative_img = Image.open(os.path.join(negative_song_dir, negative_img_name)).convert('RGB')\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            positive_img = self.transform(positive_img)\n",
    "            negative_img = self.transform(negative_img)\n",
    "        print(f\"ìƒ˜í”Œ ë¡œë”© ì‹œê°„: {time.time() - start:.4f}ì´ˆ\")\n",
    "        return anchor_img, positive_img, negative_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "957b551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì…€ 4: ì „ì²˜ë¦¬, ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì„¤ì •\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = TripletDataset(root_dir=data_root, song_ids=train_ids, transform=transform)\n",
    "val_dataset = TripletDataset(root_dir=data_root, song_ids=val_ids, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,      # ğŸš€ ë³‘ë ¬ ì´ë¯¸ì§€ ë¡œë”©\n",
    "    pin_memory=True     # ğŸš€ GPU ì „ì†¡ ì†ë„ ìµœì í™”\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2,      # ê²€ì¦ ë°ì´í„°ëŠ” ì¡°ê¸ˆ ì ì–´ë„ OK\n",
    "    pin_memory=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110c5b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triplet(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (anchor, positive, negative) in enumerate(data_loader):\n",
    "        print(f\"Batch {i+1} ì‹œì‘\")  # ë°°ì¹˜ ë²ˆí˜¸ ì¶œë ¥\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        anchor_embed = model(anchor)\n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Batch {i+1} ì™„ë£Œ, Loss: {loss.item():.4f}\")  # ë°°ì¹˜ ëë‚  ë•Œ ì¶œë ¥\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_triplet(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for anchor, positive, negative in data_loader:\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        anchor_embed = model(anchor)\n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ee36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed2e344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: 12.1\n",
      "is_available: True\n",
      "Device: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA:\", torch.version.cuda)  # âœ…\n",
    "print(\"is_available:\", torch.cuda.is_available())  # âœ…\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))  # âœ…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586f1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\anaconda3\\envs\\mynev\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\anaconda3\\envs\\mynev\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì…€ 6: í•™ìŠµ ì‹¤í–‰\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientNetEmbedding(embedding_size=128).to(device)\n",
    "loss_fn = nn.TripletMarginLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_triplet(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss = validate_triplet(model, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e69ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì…€ 7: ì„ë² ë”© ì¶”ì¶œ ë° ì¶”ì²œ í•¨ìˆ˜\n",
    "def extract_embeddings(model, inputs, device, batch_size=64):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        if isinstance(inputs, DataLoader):\n",
    "            for batch in inputs:\n",
    "                batch = batch.to(device)\n",
    "                emb = model(batch)\n",
    "                embeddings.append(emb.cpu().numpy())\n",
    "            embeddings = np.vstack(embeddings)\n",
    "        else:\n",
    "            inputs = inputs.to(device)\n",
    "            emb = model(inputs)\n",
    "            embeddings = emb.cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "def recommend_topk(query_embedding, gallery_embeddings, gallery_ids, topk=5):\n",
    "    sims = cosine_similarity(query_embedding.reshape(1, -1), gallery_embeddings).flatten()\n",
    "    topk_idx = sims.argsort()[::-1][:topk]\n",
    "    return [(gallery_ids[i], sims[i]) for i in topk_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0fd86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì…€ 8: ì¶”ë¡  ë° ì¶”ì²œ ì‹¤í–‰\n",
    "gallery_ids = train_ids + val_ids\n",
    "gallery_dataset = TripletDataset(root_dir=data_root, song_ids=gallery_ids, transform=transform)\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "gallery_embeddings = extract_embeddings(model, gallery_loader, device)\n",
    "np.save(\"gallery_embeddings.npy\", gallery_embeddings)\n",
    "\n",
    "test_img_path = r\"C:\\ts_data_triplet_split\\test\"\n",
    "test_img = Image.open(test_img_path).convert('RGB')\n",
    "test_img_tensor = transform(test_img).unsqueeze(0)\n",
    "query_embedding = extract_embeddings(model, test_img_tensor, device)\n",
    "\n",
    "gallery_id_names = [os.path.basename(d) for d, _ in gallery_dataset.data]\n",
    "recommendations = recommend_topk(query_embedding, gallery_embeddings, gallery_id_names, topk=5)\n",
    "\n",
    "print(\"ğŸ§ ì¶”ì²œ ê²°ê³¼:\")\n",
    "for i, (song_id, score) in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. ê³¡ ID: {song_id} (ìœ ì‚¬ë„: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3c392",
   "metadata": {},
   "source": [
    "## **ì „ì²´ ì½”ë“œ... íŒŒì¼ êµ¬ì¡° ë‚˜ëˆ ì•¼í•¨..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\anaconda3\\envs\\mynev\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\anaconda3\\envs\\mynev\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.8948, Val Loss: 0.8582\n",
      "Epoch 2/10, Train Loss: 0.8108, Val Loss: 0.7988\n",
      "Epoch 3/10, Train Loss: 0.7629, Val Loss: 0.7507\n",
      "Epoch 4/10, Train Loss: 0.7410, Val Loss: 0.7102\n",
      "Epoch 5/10, Train Loss: 0.7000, Val Loss: 0.6907\n",
      "Epoch 6/10, Train Loss: 0.6994, Val Loss: 0.7181\n",
      "Epoch 7/10, Train Loss: 0.6782, Val Loss: 0.6610\n",
      "Epoch 8/10, Train Loss: 0.6557, Val Loss: 0.6797\n",
      "Epoch 9/10, Train Loss: 0.6722, Val Loss: 0.6530\n",
      "Epoch 10/10, Train Loss: 0.6613, Val Loss: 0.6443\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì…€ 1: ëª¨ë“ˆ import ë° ê²½ë¡œ ì„¤ì •\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "data_root = r\"C:\\ts_data_triplet_split\\train\"\n",
    "song_ids = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))])\n",
    "random.seed(42)\n",
    "random.shuffle(song_ids)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(song_ids) * split_ratio)\n",
    "train_ids = song_ids[:split_index]\n",
    "val_ids = song_ids[split_index:]\n",
    "\n",
    "# âœ… ì…€ 2: EfficientNet ì„ë² ë”© ëª¨ë¸ ì •ì˜\n",
    "class EfficientNetEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super().__init__()\n",
    "        self.base_model = models.efficientnet_b0(pretrained=True)\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.features = self.base_model.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.embedding = nn.Linear(1280, embedding_size)\n",
    "        self.l2_norm = nn.functional.normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x = self.l2_norm(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# âœ… ì…€ 3: Triplet Dataset í´ë˜ìŠ¤ ì •ì˜\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, song_ids=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if song_ids is None:\n",
    "            self.song_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        else:\n",
    "            self.song_dirs = [os.path.join(root_dir, d) for d in song_ids]\n",
    "        self.data = []\n",
    "        for song_dir in self.song_dirs:\n",
    "            images = [f for f in os.listdir(song_dir) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "            if len(images) >= 2:\n",
    "                self.data.append((song_dir, images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(images) for _, images in self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = time.time()\n",
    "\n",
    "        anchor_song_idx = random.randint(0, len(self.data) - 1)\n",
    "        anchor_song_dir, anchor_images = self.data[anchor_song_idx]\n",
    "        anchor_img_name = random.choice(anchor_images)\n",
    "        positive_img_name = anchor_img_name\n",
    "        while positive_img_name == anchor_img_name:\n",
    "            positive_img_name = random.choice(anchor_images)\n",
    "        negative_song_idx = anchor_song_idx\n",
    "        while negative_song_idx == anchor_song_idx:\n",
    "            negative_song_idx = random.randint(0, len(self.data) - 1)\n",
    "        negative_song_dir, negative_images = self.data[negative_song_idx]\n",
    "        negative_img_name = random.choice(negative_images)\n",
    "\n",
    "        anchor_img = Image.open(os.path.join(anchor_song_dir, anchor_img_name)).convert('RGB')\n",
    "        positive_img = Image.open(os.path.join(anchor_song_dir, positive_img_name)).convert('RGB')\n",
    "        negative_img = Image.open(os.path.join(negative_song_dir, negative_img_name)).convert('RGB')\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            positive_img = self.transform(positive_img)\n",
    "            negative_img = self.transform(negative_img)\n",
    "        return anchor_img, positive_img, negative_img\n",
    "\n",
    "# âœ… ì…€ 4: ì „ì²˜ë¦¬, ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì„¤ì •\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = TripletDataset(root_dir=data_root, song_ids=train_ids, transform=transform)\n",
    "val_dataset = TripletDataset(root_dir=data_root, song_ids=val_ids, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,      # ğŸš€ ë³‘ë ¬ ì´ë¯¸ì§€ ë¡œë”©\n",
    "    pin_memory=True     # ğŸš€ GPU ì „ì†¡ ì†ë„ ìµœì í™”\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,      # ê²€ì¦ ë°ì´í„°ëŠ” ì¡°ê¸ˆ ì ì–´ë„ OK\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "def train_triplet(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (anchor, positive, negative) in enumerate(data_loader):\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        anchor_embed = model(anchor)\n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_triplet(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for anchor, positive, negative in data_loader:\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        anchor_embed = model(anchor)\n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# âœ… ì…€ 6: í•™ìŠµ ì‹¤í–‰ -- 6ë²ˆê¹Œì§„ ì˜ ëŒì•„ê°. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientNetEmbedding(embedding_size=128).to(device)\n",
    "loss_fn = nn.TripletMarginLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "''' í•™ìŠµ ì™„ë£Œì‹œ í•„ìš”ì—†ìŒ.\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_triplet(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss = validate_triplet(model, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "'''\n",
    "\n",
    "#í•™ìŠµ ì™„ë£Œì‹œ ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°.\n",
    "model.load_state_dict(torch.load(\"model_epoch_10.pth\", map_location=device))  # íŒŒì¼ëª… í™•ì¸\n",
    "model.eval()\n",
    "\n",
    "# âœ… ì…€ 7: ì„ë² ë”© ì¶”ì¶œ ë° ì¶”ì²œ í•¨ìˆ˜\n",
    "def extract_embeddings(model, inputs, device, batch_size=64):\n",
    "    print(\"[DEBUG] â–¶ ì„ë² ë”© ì¶”ì¶œ ì‹œì‘\")\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        if isinstance(inputs, DataLoader):\n",
    "            if len(inputs.dataset) == 0:\n",
    "                print(\"[ERROR] âš ï¸ DataLoaderì˜ Datasetì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "                return np.array([])  # ğŸ‘ˆ ë¹ˆ ë°°ì—´ ë¦¬í„´í•˜ê±°ë‚˜ ì¢…ë£Œ ì²˜ë¦¬\n",
    "            \n",
    "            print(f\"[DEBUG] â–¶ inputs ê¸¸ì´: {len(inputs.dataset)}\")\n",
    "            for batch in inputs:\n",
    "                print(f\"[DEBUG] â–¶ ë°°ì¹˜ {i+1} ì¶”ì¶œ ì¤‘...\")\n",
    "                batch = batch.to(device)\n",
    "                emb = model(batch)\n",
    "                print(f\"[DEBUG] â–¶ ë°°ì¹˜ {i+1} ì„ë² ë”© shape: {emb.shape}\")\n",
    "                embeddings.append(emb.cpu().numpy())\n",
    "            embeddings = np.vstack(embeddings)\n",
    "            print(f\"[DEBUG] â–¶ ì „ì²´ ì„ë² ë”© shape: {embeddings.shape}\")\n",
    "        else:\n",
    "            print(\"[DEBUG] â–¶ ë‹¨ì¼ ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ\")\n",
    "            inputs = inputs.to(device)\n",
    "            emb = model(inputs)\n",
    "            print(f\"[DEBUG] â–¶ ë‹¨ì¼ ì„ë² ë”© shape: {emb.shape}\")\n",
    "            embeddings = emb.cpu().numpy()\n",
    "    print(\"[DEBUG] â–¶ ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ\")\n",
    "    return embeddings\n",
    "\n",
    "def recommend_topk(query_embedding, gallery_embeddings, gallery_ids, topk=5):\n",
    "    sims = cosine_similarity(query_embedding.reshape(1, -1), gallery_embeddings).flatten()\n",
    "    topk_idx = sims.argsort()[::-1][:topk]\n",
    "    return [(gallery_ids[i], sims[i]) for i in topk_idx]\n",
    "\n",
    "# âœ… ì…€ 7,8ì‚¬ì´ : ì‹±ê¸€ë°ì´í„°ì…‹ ë§Œë“œëŠ” í´ë˜ìŠ¤ í•„ìš”.\n",
    "class SingleImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, song_ids=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.image_ids = []\n",
    "        song_dirs = [os.path.join(root_dir, d) for d in song_ids]\n",
    "        for song_dir in song_dirs:\n",
    "            images = [f for f in os.listdir(song_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            for img in images:\n",
    "                self.image_paths.append(os.path.join(song_dir, img))\n",
    "                self.image_ids.append(os.path.basename(song_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(f\"[DEBUG] â–¶ ì´ë¯¸ì§€ ë¡œë“œ ì‹œì‘: {self.image_paths[idx]}\")\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "# âœ… ì…€ 8: ì¶”ë¡  ë° ì¶”ì²œ ì‹¤í–‰\n",
    "# âœ… ê°¤ëŸ¬ë¦¬ ë°ì´í„° ì¤€ë¹„ ë° ì„ë² ë”© ì¶”ì¶œ. í•œë²ˆë§Œ í•´ì„œ ì €ì¥í•´ë†¨ë‹¤ê°€ ì¶”ì²œí•  ë•Œ ì‚¬ìš©.\n",
    "gallery_ids = train_ids + val_ids\n",
    "gallery_dataset = SingleImageDataset(root_dir=data_root, song_ids=gallery_ids, transform=transform)\n",
    "\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "gallery_embeddings = extract_embeddings(model, gallery_loader, device)\n",
    "np.save(\"gallery_embeddings.npy\", gallery_embeddings)\n",
    "\n",
    "# âœ… ê°¤ëŸ¬ë¦¬ ID ì´ë¦„ ì¶”ì¶œ\n",
    "gallery_id_names = [os.path.basename(d) for d, _ in gallery_dataset.data]\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì§€ì •\n",
    "test_img_dir = r\"C:\\ts_data_triplet_split\\test\"\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤ ì¤‘ ë¬´ì‘ìœ„ë¡œ 10ê°œ ì„ íƒ\n",
    "all_test_paths = [\n",
    "    os.path.join(test_img_dir, f)\n",
    "    for f in os.listdir(test_img_dir)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]\n",
    "random.seed(42)\n",
    "test_img_paths = random.sample(all_test_paths, min(10, len(all_test_paths)))\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í•˜ë‚˜ì”© ì¶”ì²œ ìˆ˜í–‰\n",
    "for test_img_path in test_img_paths:\n",
    "    print(f\"\\n[DEBUG] â–¶ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {test_img_path}\")\n",
    "    test_img = Image.open(test_img_path).convert('RGB')\n",
    "    test_img_tensor = transform(test_img).unsqueeze(0)  # (1, C, H, W)\n",
    "    query_embedding = extract_embeddings(model, test_img_tensor, device)\n",
    "\n",
    "    print(\"[DEBUG] â–¶ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\")\n",
    "    recommendations = recommend_topk(query_embedding, gallery_embeddings, gallery_id_names, topk=5)\n",
    "\n",
    "    print(f\"\\nğŸ§ [{os.path.basename(test_img_path)}]ì— ëŒ€í•œ ì¶”ì²œ ê²°ê³¼:\")\n",
    "    for i, (song_id, score) in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. ê³¡ ID: {song_id} (ìœ ì‚¬ë„: {score:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
